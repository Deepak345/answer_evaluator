{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('q1.csv')\n",
    "dataset= dataset[dataset['ans'].notna()]#ignore NAN values\n",
    "# print(dataset.head())\n",
    "# print(len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(dataset,test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   img_id                                                ans  marks\n",
      "76  IMG77  ସମାନତା ସନ୍ଧି ପ୍ରଥା ଜେନେରାଲ ଡେଲହାଉସି ପ୍ରଣୟନ କରି...    4.5\n",
      "39  IMG40  ଭାରତରେ ଇଂଗ୍ରେଜ଼ ମାନଙ୍କ ପ୍ରଭାବ ବିସ୍ତାର କରିବା ପା...    4.0\n",
      "0   IMG01  ଇଂଗରେଜକଂ ସାମରାଜ୍ୟ କୁ ସୁଦୃଢ କରିବା ପାଇଁ ୱେଲସେଲି ...    5.0\n",
      "79  IMG80  ସାମନ୍ତ ସନ୍ଧି ୱେଲସେଲି ପ୍ରଣୟନ କରିଥଲେ ଇଂଗ୍ରେଜ଼ ମା...    4.5\n",
      "46  IMG47  ସାମନ୍ତ ସନ୍ଧିକୁ ଇଂଗ୍ରେଜ଼ ଲର୍ଡ଼ ଡେଲହାଉସି ପ୍ରଚଳନ ...    0.5\n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   img_id                                                ans  marks\n",
      "48  IMG49  ଭାରତରେ ସାମ୍ରାଜ୍ୟ ବିସ୍ତାର କରିବ ତଥା ବାଣିଜ୍ୟ ପ୍ରସ...    2.0\n",
      "21  IMG22  ଇଂଗ୍ରେଜ଼ ମାନଂକ ବିନା ଅନୁମତିରେ କୌଣସି ଦେଶ ସହ ବାଣି...    3.0\n",
      "5   IMG06  ଭାରତରେ ଇଂଗ୍ରେଜ଼ ମାନଂକ ସାମ୍ରାଜ୍ୟ ବିସ୍ତାର ପାଇଁ ୱ...    5.0\n",
      "10  IMG11  ୱେଲସେଲି ସାମନ୍ତ ସନ୍ଧି ପ୍ରଚଳନ କରିଥିଲେ|ତାଙ୍କର ଅନ୍...    2.0\n",
      "58  IMG59  ଭାରତରେ ଇଂଗ୍ରେଜ଼ ମାନଙ୍କ ସାମ୍ରାଜ୍ୟ ବୃଦ୍ଧି ପାଇଁ ୱ...    5.0\n",
      "52  IMG53  ଭାରତରେ ଇଂଗ୍ରେଜ଼ ମାନଙ୍କ ପ୍ରଭୁତ୍ୱ ବିସ୍ତାର କାରିବା...    4.5\n",
      "78  IMG79  ଇଂଗ୍ରେଜ଼ ମାନେ ସାମ୍ରାଜ୍ୟ ବୃଦ୍ଧି କରିବା ପାଇଁ ୱେଲସ...    4.0\n",
      "6   IMG07  ୱେଲସେଲି ଇଂଗ୍ରେଜ଼ ଶାସନର ପ୍ରସାର ପାଇଁଏକ ପ୍ରଥା ପ୍ର...    5.0\n",
      "81  IMG82  ଇଂଗ୍ରେଜ଼ ମାନେ ଭାରତରେ ବାଣିଜ୍ୟ କରିବାକୁ ଆସିଥିଲେ|ଭ...    2.0\n",
      "16  IMG17  ଭାରତରେ ରାଜ୍ୟ ବିସ୍ତାର କରିବା ପାଇଁ ଲର୍ଡ଼ ୱେଲସେଲି ...    5.0\n",
      "34  IMG35  ସାମନ୍ତ ସନ୍ଧି ପ୍ରଥା ପ୍ରଚଳନ କରିଥିଲେ କାରଣ ସେ ଭାରତ...    0.0\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import tokenize,get_embedding_vectors #importing tokenizer using inltk\n",
    "def sentence_tokenization(answer):\n",
    "    token = []\n",
    "    for ans in answer:\n",
    "        token.append(tokenize(str(ans),'or'))\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of training set\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "for ans in train['ans']:\n",
    "    answer.append(ans)\n",
    "token=[]\n",
    "print(\"Tokenization of training set\")\n",
    "token = sentence_tokenization(answer)\n",
    "# print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "        level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    num_features = 400    # Word vector dimensionality\n",
    "    min_word_count = 5   # Minimum word count\n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    window = 5         # Context window size\n",
    "    downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 01:58:12,580 : INFO : collecting all words and their counts\n",
      "2020-05-03 01:58:12,581 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-05-03 01:58:12,584 : INFO : collected 662 word types from a corpus of 6956 raw words and 56 sentences\n",
      "2020-05-03 01:58:12,585 : INFO : Loading a fresh vocabulary\n",
      "2020-05-03 01:58:12,587 : INFO : effective_min_count=5 retains 199 unique words (30% of original 662, drops 463)\n",
      "2020-05-03 01:58:12,588 : INFO : effective_min_count=5 leaves 6186 word corpus (88% of original 6956, drops 770)\n",
      "2020-05-03 01:58:12,590 : INFO : deleting the raw counts dictionary of 662 items\n",
      "2020-05-03 01:58:12,591 : INFO : sample=0.001 downsamples 93 most-common words\n",
      "2020-05-03 01:58:12,592 : INFO : downsampling leaves estimated 3157 word corpus (51.0% of prior 6186)\n",
      "2020-05-03 01:58:12,594 : INFO : estimated required memory for 199 words and 400 dimensions: 736300 bytes\n",
      "2020-05-03 01:58:12,595 : INFO : resetting layer weights\n",
      "2020-05-03 01:58:12,663 : INFO : training model with 4 workers on 199 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-03 01:58:12,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-03 01:58:12,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-03 01:58:12,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-03 01:58:12,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-03 01:58:12,685 : INFO : EPOCH - 1 : training on 6956 raw words (3059 effective words) took 0.0s, 214796 effective words/s\n",
      "2020-05-03 01:58:12,691 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-03 01:58:12,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-03 01:58:12,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-03 01:58:12,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-03 01:58:12,702 : INFO : EPOCH - 2 : training on 6956 raw words (3146 effective words) took 0.0s, 276215 effective words/s\n",
      "2020-05-03 01:58:12,707 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-03 01:58:12,708 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-03 01:58:12,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-03 01:58:12,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-03 01:58:12,718 : INFO : EPOCH - 3 : training on 6956 raw words (3195 effective words) took 0.0s, 259408 effective words/s\n",
      "2020-05-03 01:58:12,723 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-03 01:58:12,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-03 01:58:12,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-03 01:58:12,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-03 01:58:12,735 : INFO : EPOCH - 4 : training on 6956 raw words (3110 effective words) took 0.0s, 219887 effective words/s\n",
      "2020-05-03 01:58:12,746 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-03 01:58:12,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-03 01:58:12,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-03 01:58:12,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-03 01:58:12,761 : INFO : EPOCH - 5 : training on 6956 raw words (3096 effective words) took 0.0s, 166064 effective words/s\n",
      "2020-05-03 01:58:12,763 : INFO : training on a 34780 raw words (15606 effective words) took 0.1s, 158915 effective words/s\n",
      "2020-05-03 01:58:12,763 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-05-03 01:58:12,764 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-05-03 01:58:12,766 : INFO : saving Word2Vec object under word2vec_model, separately None\n",
      "2020-05-03 01:58:12,767 : INFO : not storing attribute vectors_norm\n",
      "2020-05-03 01:58:12,767 : INFO : not storing attribute cum_table\n",
      "2020-05-03 01:58:12,777 : INFO : saved word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "MODEL Word2Vec(vocab=199, size=400, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#     print(\"Training Word2Vec model...\")\n",
    "#     model = Word2Vec(token, workers=num_workers, \\\n",
    "#                 size=num_features, min_count = min_word_count, \\\n",
    "#                 window = window, sample = downsampling, seed=1)\n",
    "\n",
    "#     # If you don't plan to train the model Any further, calling\n",
    "#     # init_sims will make the model much more memory-efficient.\n",
    "#     model.init_sims(replace=True)\n",
    "#     print(\"MODEL\",model)\n",
    "#     # It can be helpful to create a meaningful model name and\n",
    "#     # save the model for later use. You can load it later using Word2Vec.load()\n",
    "#     model_name = \"word2vec_model\"\n",
    "#     model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVec(words, model, num_features):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the ans and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "#     print(featureVec)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgFeatureVecs(reviews, model, num_features):\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 10th review\n",
    "       if counter%10. == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       #\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[int(counter)] = featureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    print(reviewFeatureVecs)\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 56\n",
      "Review 10 of 56\n",
      "Review 20 of 56\n",
      "Review 30 of 56\n",
      "Review 40 of 56\n",
      "Review 50 of 56\n",
      "[[ 0.01893   0.031015  0.013522 -0.03856  ...  0.012261 -0.043516  0.034974  0.046051]\n",
      " [ 0.018951  0.03109   0.013488 -0.038462 ...  0.012214 -0.04347   0.034959  0.046107]\n",
      " [ 0.018916  0.031144  0.013583 -0.038507 ...  0.01215  -0.043496  0.034871  0.046188]\n",
      " [ 0.018877  0.030978  0.013518 -0.038557 ...  0.012275 -0.043537  0.034934  0.046087]\n",
      " ...\n",
      " [ 0.018962  0.031086  0.013452 -0.038442 ...  0.012161 -0.043488  0.035098  0.045973]\n",
      " [ 0.019039  0.030906  0.013539 -0.038622 ...  0.012129 -0.043519  0.035029  0.045999]\n",
      " [ 0.01896   0.031014  0.013548 -0.038417 ...  0.012293 -0.043424  0.034947  0.046112]\n",
      " [ 0.019005  0.031014  0.013469 -0.038522 ...  0.012222 -0.043547  0.03495   0.046089]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamsid2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# trainVec = avgFeatureVecs( token, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01893   0.031015  0.013522 -0.03856  ...  0.012261 -0.043516  0.034974  0.046051]\n",
      " [ 0.018951  0.03109   0.013488 -0.038462 ...  0.012214 -0.04347   0.034959  0.046107]\n",
      " [ 0.018916  0.031144  0.013583 -0.038507 ...  0.01215  -0.043496  0.034871  0.046188]\n",
      " [ 0.018877  0.030978  0.013518 -0.038557 ...  0.012275 -0.043537  0.034934  0.046087]\n",
      " ...\n",
      " [ 0.018962  0.031086  0.013452 -0.038442 ...  0.012161 -0.043488  0.035098  0.045973]\n",
      " [ 0.019039  0.030906  0.013539 -0.038622 ...  0.012129 -0.043519  0.035029  0.045999]\n",
      " [ 0.01896   0.031014  0.013548 -0.038417 ...  0.012293 -0.043424  0.034947  0.046112]\n",
      " [ 0.019005  0.031014  0.013469 -0.038522 ...  0.012222 -0.043547  0.03495   0.046089]]\n"
     ]
    }
   ],
   "source": [
    "# print(trainVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(trainVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inltkWordVec(asnwer):\n",
    "    wordVec = []\n",
    "    for ans in answer:\n",
    "        wordVec.append(get_embedding_vectors(str(t),'or'))\n",
    "    return wordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# itrainVec = inltkWordVec(answer)\n",
    "# print(itrainVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in itrainVec:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of test set\n"
     ]
    }
   ],
   "source": [
    "answer_test = []\n",
    "for ans in test['ans']:\n",
    "    answer_test.append(str(ans))\n",
    "token_test=[]\n",
    "print(\"Tokenization of test set\")\n",
    "token_test = sentence_tokenization(answer_test)\n",
    "# print(token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 11\n",
      "Review 10 of 11\n",
      "[[ 0.019078  0.03101   0.013404 -0.038566 ...  0.012094 -0.04347   0.035063  0.046147]\n",
      " [ 0.019006  0.031069  0.013403 -0.038459 ...  0.012283 -0.043509  0.034956  0.046052]\n",
      " [ 0.018999  0.031042  0.013469 -0.038472 ...  0.01228  -0.043454  0.034968  0.046105]\n",
      " [ 0.018898  0.031005  0.013498 -0.038683 ...  0.012263 -0.043543  0.034936  0.046063]\n",
      " ...\n",
      " [ 0.018981  0.031034  0.013548 -0.038469 ...  0.012213 -0.043485  0.034988  0.046098]\n",
      " [ 0.01908   0.031007  0.013487 -0.03851  ...  0.012207 -0.043508  0.035006  0.046047]\n",
      " [ 0.018999  0.030961  0.013443 -0.038511 ...  0.012177 -0.04351   0.034986  0.046065]\n",
      " [ 0.019202  0.031122  0.013693 -0.038508 ...  0.012093 -0.043376  0.034953  0.046018]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamsid2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "testVec = avgFeatureVecs(token_test, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itestVec = inltkWordVec(answer_test)\n",
    "# print(itestVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Length Vec Test\",len(itestVec))\n",
    "# print(\"Gensim Length\")\n",
    "# for t in testVec:\n",
    "#       print(len(t))\n",
    "# print(\"INLTK Length\")\n",
    "# for l in itestVec:\n",
    "#     print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(l, content, width):\n",
    "    l.extend([content] * (width - len(l)))\n",
    "    return l\n",
    "\n",
    "# pad(itrainVec,0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainVec, train[\"marks\"].astype(int))\n",
    "\n",
    "# Test & extract results\n",
    "# result = forest.predict(np.array(xtestVec,dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test & extract results\n",
    "result = forest.predict(testVec)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Mark\n",
      "[2 3 5 2 5 4 4 5 2 5 0]\n",
      "Predicted Mark\n",
      "[4 5 5 4 5 5 4 5 5 5 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate accuracy\n",
    "\n",
    "test_mark = []\n",
    "for i in test['marks'].astype(int):\n",
    "    test_mark.append(i)\n",
    "print(\"Actual Mark\")\n",
    "test_mark = np.array(test_mark)\n",
    "print(test_mark)\n",
    "print(\"Predicted Mark\")\n",
    "print(result)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_mark, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CNN model\n",
    "#step1-importing tf\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
